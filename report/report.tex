\documentclass[a4paper, oneside]{memoir}
% Fixes "No room for a new \xxx" error by extending the default 256 fixed size
% LaTeX arrays
\usepackage{etex}
%\reserveinserts{28}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[british]{babel}

% bedre orddeling Gør at der som minimum skal blive to tegn på linien ved
% orddeling og minimum flyttes to tegn ned på næste linie. Desværre er værdien
% anvendt af babel »12«, hvilket kan give orddelingen »h-vor«.
\renewcommand{\britishhyphenmins}{22}

% Fix of fancyref to work with memoir. Makes references look
% nice. Redefines memoir \fref and \Fref to \refer and \Refer.
% \usepackage{refer} %
% As we dont really have any use for \fref and \Fref we just undefine what
% memoir defined them as, so fancyref can define what it wants.
\let\fref\undefined
\let\Fref\undefined
\usepackage{fancyref} % Better reference.

\usepackage{pdflscape} % Gør landscape-environmentet tilgængeligt
\usepackage[draft]{fixme}     % Indsæt "fixme" noter i drafts.
\usepackage{hyperref}  % Indsæter links (interne og eksterne) i PDF

\usepackage{mdwtab}
\usepackage{mathenv}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{semantic} % for the \mathlig function

\usepackage[format=hang]{caption,subfig}
\usepackage{graphicx}
\usepackage{stmaryrd}
\usepackage[final]{listings} % Make sure we show the listing even though we are
                             % making a final report.

\usepackage{ulem} % \sout - strike-through
% ulem changes \emph to be underline instead of being italic.
% So we change it back to be italic.
\normalem

\usepackage{tikz}

\lstset{ %
% language=Octave,                % choose the language of the code
basicstyle=\ttfamily,        % the size of the fonts that are used for the code
basewidth=0.5em,
% numbers=left,                   % where to put the line-numbers
% numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
% stepnumber=2,                   % the step between two line-numbers. If it's 1 each line will be numbered
% numbersep=5pt,                  % how far the line-numbers are from the code
% backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
% showspaces=false,               % show spaces adding particular underscores
% showstringspaces=false,         % underline spaces within strings
% showtabs=false,                 % show tabs within strings adding particular underscores
% frame=single	                % adds a frame around the code
% tabsize=2,	                % sets default tabsize to 2 spaces
% captionpos=b,                   % sets the caption-position to bottom
% breaklines=true,                % sets automatic line breaking
% breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
escapeinside={(@}{@)}          % if you want to add a comment within your code
}

\renewcommand{\ttdefault}{txtt} % Bedre typewriter font
%\usepackage[sc]{mathpazo}     % Palatino font
\renewcommand{\rmdefault}{ugm} % Garamond
%\usepackage[garamond]{mathdesign}

%\overfullrule=5pt
%\setsecnumdepth{part}
\setcounter{secnumdepth}{1} % Sæt overskriftsnummereringsdybde. Disable = -1.

% fix to make this look like an article.. Really ugly but we have a 10page limit!!!!!
\chapterstyle{article} % changes style of chapters, to look nice.

\let\clearforchapter\par % Dont clear the page before each chapter. This saves
                         % pages.. 10 page limit !!!!

\makeatletter
\newenvironment{nonfloatingfigure}{
  \vskip\intextsep
  \def\@captype{figure}
  }{
  \vskip\intextsep
}

\newenvironment{nonfloatingtable}{
  \vskip\intextsep
  \def\@captype{table}
  }{
  \vskip\intextsep
}
\makeatother


\theoremstyle{definition}
\newtheorem{judgment}{Judgment}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}

\newcommand*{\fancyrefdeflabelprefix}{def}
\fancyrefaddcaptions{english}{
  \newcommand*{\Frefdefname}{Definition}
  \newcommand*{\frefdefname}{\MakeLowercase{\Frefdefname}}
}
\frefformat{vario}{\fancyrefdeflabelprefix}{%
  \frefdefname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyrefdeflabelprefix}{%
  \Frefdefname\fancyrefdefaultspacing#1#3%
}

\newcommand*{\fancyreflemlabelprefix}{lem}
\fancyrefaddcaptions{english}{
  \newcommand*{\Freflemname}{Lemma}
  \newcommand*{\freflemname}{\MakeLowercase{\Freflemname}}
}
\frefformat{vario}{\fancyreflemlabelprefix}{%
  \freflemname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyreflemlabelprefix}{%
  \Freflemname\fancyrefdefaultspacing#1#3%
}
\frefformat{plain}{\fancyreflemlabelprefix}{%
  \freflemname\fancyrefdefaultspacing#1%
}
\Frefformat{plain}{\fancyreflemlabelprefix}{%
  \Freflemname\fancyrefdefaultspacing#1%
}

\newcommand*{\fancyrefproplabelprefix}{prop}
\fancyrefaddcaptions{english}{
  \newcommand*{\Frefpropname}{Proposition}
  \newcommand*{\frefpropname}{\MakeLowercase{\Frefpropname}}
}
\frefformat{vario}{\fancyreflemlabelprefix}{%
  \freflemname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyreflemlabelprefix}{%
  \Freflemname\fancyrefdefaultspacing#1#3%
}
\frefformat{plain}{\fancyreflemlabelprefix}{%
  \freflemname\fancyrefdefaultspacing#1%
}
\Frefformat{plain}{\fancyreflemlabelprefix}{%
  \Freflemname\fancyrefdefaultspacing#1%
}

\newcommand*{\fancyrefthmlabelprefix}{thm}
\fancyrefaddcaptions{english}{
  \newcommand*{\Frefthmname}{Theorem}
  \newcommand*{\frefthmname}{\MakeLowercase{\Frefthmname}}
}
\frefformat{vario}{\fancyrefthmlabelprefix}{%
  \frefthmname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyrefthmlabelprefix}{%
  \Frefthmname\fancyrefdefaultspacing#1#3%
}

\newcommand*{\fancyrefcorlabelprefix}{cor}
\fancyrefaddcaptions{english}{
  \newcommand*{\Frefcorname}{Corollary}
  \newcommand*{\frefcorname}{\MakeLowercase{\Frefcorname}}
}
\frefformat{vario}{\fancyrefcorlabelprefix}{%
  \frefcorname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyrefcorlabelprefix}{%
  \Frefcorname\fancyrefdefaultspacing#1#3%
}

\newcommand*{\fancyrefexlabelprefix}{ex}
\fancyrefaddcaptions{english}{
  \newcommand*{\Frefexname}{Example}
  \newcommand*{\frefexname}{\MakeLowercase{\Frefexname}}
}
\frefformat{vario}{\fancyrefexlabelprefix}{%
  \frefexname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyrefexlabelprefix}{%
  \Frefexname\fancyrefdefaultspacing#1#3%
}
\frefformat{plain}{\fancyrefexlabelprefix}{%
  \frefexname\fancyrefdefaultspacing#1%
}
\Frefformat{plain}{\fancyrefexlabelprefix}{%
  \Frefexname\fancyrefdefaultspacing#1%
}

\newcommand{\ttt}[1]{\texttt{#1}}
\newcommand{\tnm}[1]{\textnormal{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}

\newcommand{\Cov}{\mathrm{Cov}}
\providecommand{\FV}{\mathrm{FV}}
\providecommand{\Dom}{\mathrm{Dom}}

\mathlig{||}{\parallel}
\mathlig{<'}{\prec}
\mathlig{>'}{\succ}
\mathlig{<='}{\preccurlyeq}
\mathlig{>='}{\succcurlyeq}
\mathlig{<=}{\leqslant}
\mathlig{>=}{\geqslant}
\mathlig{<>}{\neq}
\mathlig{|=}{\sqsubset}
\mathlig{=|}{\sqsupset}
\mathlig{==}{\equiv}
\mathlig{==a}{=_{\alpha}}
\mathlig{<|}{\lhd}
\mathlig{|>}{\rhd}
\mathlig{++}{\mathrel{\mbox{+\!\!\!+}}}
% ~>e or ~>g conflicts with the \cite command for some reason.
\mathlig{->e}{\stackrel{elim}{\leadsto}}
\mathlig{->g}{\stackrel{gen}{\leadsto}}
\mathlig{->n}{\leadsto}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	    	     Forside
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\makeatletter % open mode for reading @ signed variables 
\def\maketitle{%
  \null
  \thispagestyle{empty}%
  \vfill
  \begin{center}\leavevmode
    % \normalfont
    % \Huge{\raggedleft \@title\par}%
    % \hrulefill\par
    % \Large{\raggedright \subtitle\par}%
    \begin{flushleft}
      \huge \@title
    \end{flushleft}
    \par
    \hrule height 1pt
    \par
    \begin{flushright}
      \large \subtitle \par
      \vspace{2pt}
      \footnotesize DIKU \@date
    \end{flushright}
    \vskip 10pt
    % {\@date\par}%
    \begin{quote}
      \textit{\descript}
    \end{quote}
  \end{center}%
  \vfill
\begin{minipage}{80pt}
\includegraphics*[scale=0.75]{imgs/nat-logo}
\end{minipage}
\begin{minipage}{300pt}
  \begin{flushleft}
    {\large \@author } \\
    {\footnotesize \suplementInfo }

  \end{flushleft}
\end{minipage}
\cleardoublepage % lave 1 ekstre side blank efter
  \clearpage % Terminates the page here. Everything else vil be placed on next page.
}
\makeatother % closing mode for reading @ signed variables
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		Data til forside
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Regular-expression based bit coding}
\def\subtitle{TiPL - Topics in Programming Languages.}
\author{Morten Brøns-Pedersen {\footnotesize (mortenbp@gmail.com)}\\
Jesper Reenberg {\footnotesize (jesper.reenberg@gmail.com)} \\
Nis Wegmann {\footnotesize (niswegmann@gmail.com)}}
\def\descript{Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent lobortis quam at dolor adipiscing dictum. Proin sit amet nunc elit. Donec tempor eros vitae quam posuere nec ullamcorper ligula tincidunt. Suspendisse condimentum venenatis quam, a imperdiet elit consectetur id. Duis vitae dignissim enim. Donec molestie mattis varius. Nunc eget sapien sit amet nibh pulvinar gravida sed vitae nunc. Suspendisse pharetra auctor molestie. Sed vel ipsum vitae quam elementum ultrices ut a lacus. Proin ut mi nibh, nec tempus dolor. Fusce ac ante sit amet nisi porttitor vehicula in at elit. Maecenas semper tellus nec leo viverra blandit. Suspendisse ullamcorper pulvinar nulla ut feugiat. Vivamus nec elementum massa. Donec quis nisl sed justo euismod blandit. Vivamus sit amet erat a turpis porttitor mollis. Sed cursus tortor vitae nulla ullamcorper porta. Quisque sapien dolor, tempor eget rutrum id, cursus vel ipsum.}

\def\suplementInfo{
  \kern 5pt \hrule width 11pc \kern 5pt % putter 5pt spacing oven over og neden under stregen
  Dept. of Computer Science \\
  University of Copenhagen}
% \date{} % used to set explicit dates

\pagestyle{plain}

\begin{document}

\frontmatter

\maketitle
\thispagestyle{empty}

\clearpage

\tableofcontents*

\mainmatter

\fixme{Replace ugly amarican z's with proper English} 
\fixme{refered is spelled referred as by url in comment (latex source)}

%\url{http://dictionary.cambridge.org/dictionary/british/refer-sb-to-sth_2#refer-sb-to-sth_2__2}

\chapter{Introduction}

The Extensible Markup Language (XML) \cite{w3c2000} is an extensively used
format for encoding documents and storing data in a machine-readable
form. During the last decade hundreds of XML-based domain specific languages has
emerged, e.g. RSS for publishing web blogs, XHTML for encoding web pages, CML
for managing molecular information, SVG for describing two-dimensional vector
graphics, and OpenDocument for representing electronic documents. Further, XML
has become ubiquitous in both commercial industries such as banking and
telecommunication \cite{lisu2000}, and in science and research industries
\cite{zhlu2002, gi2000}.

The advantage of using XML is flexibility and access to the vast amount of
existing tools and technologies tailored at XML. Nonetheless encoding data
in XML is highly inefficient in terms of space-usage, as large amounts of
meta-data are replicated throughout the encoded file; hence tools for compressing
XML encoded data is of main concern.

Existing general data-compressing techniques are typically based on a statistical
approach using codebooks. In this paper we take another approach and investigate
a method specific for XML files which exploits the tidy semantics of the XML-format.
Given an XML scheme file \cite{wa2001}, we automatically derive a regular
expression which matches all instances of the XML scheme. Using this regular
expression enables us to parse an instance of the XML scheme, and we encode
the parse tree efficiently using a bitcoding technique which only encode
non-deterministic parts of the parse tree. We have implemented a prototype
of our method in Haskell \cite{sugost2008} and conducted experiments using
this prototype for compressing data obtained from the Digital Bibliography \& Library Project .

Existing general data-compressing techniques are typically based on a
statistical approach using codebooks. In this paper we take another approach and
investigate a method specific for XML files which exploits the tidy semantics of
the XML-format. Given an XML scheme file \cite{wa2001}, we automatically derive
a regular expression which matches all instances of the XML scheme. Using this
regular expression enables us to parse an instance of the XML scheme, and we
encode the parse tree efficiently using a bit coding technique which only encode
non-deterministic choices in the parse tree. We have implemented a prototype of
our method in the declarative language Haskell \cite{sugost2008} and conducted
experiments using this prototype for compressing data obtained from the Digital
Bibliography \& Library Project.

This paper marks the conclusion of the 2010 candidate course \emph{Topics in
  Programming Languages} at Department of Computer Science, Copenhagen
University.

\section{Prerequisites}

We expect readers to posses basic knowledge of regular expressions \cite{houl2006}.
Also we expect familiarity with inference system and programming language semantics \cite{gu1992, wi1993}.
Knowledge of XML and DTD schemes is necessary to understand Section \ref{sec:convert},
and knowledge of Huffman coding \cite[Section 16.3]{co2001} is necessary to understand Section \ref{sec:specialize_huffman}.

\section{Previous work}

Our work is based on Henglein and Nielsen's paper \cite{heni2010}. In their
paper they propose a straightforward technique for encoding syntax trees parsed
under regular expressions efficiently as bit sequences. This is closely related
to the work done by Necula and Rahul \cite{nera2001} where a variant of the same
technique is used for compact encoding of proof-carrying code. By using
oracle-based checking only non-deterministic choices in proof-trees need to be
encoded.

In \cite{lisu2000} Leifke and Suciu describes their XML compressor
\emph{XMill}. By using a specialized version of \emph{zlib} \cite{dega1996} they
are able to compress XML files up to twice of the compression ratio of gzip
\cite{de1996}.

\section{Outline}

The rest of the paper is organized as follows:
Section \ref{sec:formalization} lays down the formal foundation on which we reason about regular expressions;
Section \ref{sec:convert} shows how to automatically derive a regular expression from a DTD scheme;
Section \ref{sec:mu_recursion} extends regular expressions with recursion, and shows how to encode parse trees efficienctly;
Section \ref{sec:specialize_huffman} shows how to specialize a regular expression to the specific XML-data being compressed;
Section \ref{sec:experimental_results} evaluates our prototype and presents experimental results;
Section \ref{sec:conclusion} concludes the paper and presents further work.

\section{Acknowledgement}

The authors extend their gratitude to Joakim Ahnfelt-Rønne, Jon Elverkilde, and
Michael Flænø Werk for insightful comments. Also we would like to thank Lasse
Nielsen for providing useful source code for regular expression parsing.

\chapter{Formalisation}
\label{sec:formalisation}

\begin{figure}
\[
\begin{array}{ccc}
  \inference{}{() : 1}
&
  \inference{}{\texttt{a} : \texttt{a}}
&
  \inference{v : E}{\mathtt{inl}\ v : E + F}
\\
\\
  \inference{v : F}{\mathtt{inr}\ v : E + F}
&
  \inference{v : E & w : F}{(v, w) : E \times F}
&
  \inference{v : 1 + E \times E^{\ast}}{\mathtt{fold}\ v : E^{\ast}}
\end{array}
\]
\caption{Inhabitation proofs for regular expressions.}
\label{fig:inhabitation_proofs}
\end{figure}

We define the language of regular expressions $Reg_\Sigma$ over the finite
alphabet $\Sigma = {a_1, \dots, a_n}$ as follows:

\[
    E ::= 0 \; | \; 1 \; | \; E_1 + E_2 \; | \; E_1 \times E_2 \; | \; E^{*}
\]

\noindent with $\times$ having higher precedence than $+$.

Based on the formalisation of Henglein and Nielsen \cite{heni2010} we focus on
string parsing under regular expressions. \Fref{fig:inhabitation_proofs}
shows their inference system, in which inhabitation proofs for regular
expressions can be derived. We write $\vdash v : E$ if $v : E$ is derivable in
it. Proof trees $v$, such that $\vdash v : E$ for some $E$, will be referred to
as \emph{proof values}.

The \emph{flattening} $||v||$ of a proof value $v$ is defined as follows:

\[
\begin{array}{rclrcl}
||()|| & = & \epsilon &
||a||  & = & a \\
|| \mathtt{inl} \; v|| & = & ||v|| &
|| \mathtt{inr} \; v|| & = & ||v|| \\
|| (v,w)|| & = & ||v||||w|| &
|| \mathtt{fold} \; v|| & = & ||v||
\end{array}
\]

The \emph{language} $\mathcal{L}(E)$ of the regular expression $E$ is defined by

\[
\mathcal{L}(E) = \{ s \; | \; \vdash v : E, ||v|| = s \}.
\]

Equivalence of regular expression $E_1$ and $E_2$ is denoted by $E_1 = E_2$ and
holds if $\mathcal{L}(E_1) = \mathcal{L}(E_2)$; Containment of regular
expression $E_1$ in $E_2$ is denoted by $E_1 \le E_2$ and holds if
$\mathcal{L}(E_1) \subseteq \mathcal{L}(E_2)$.  It follows from basic set theory
that $E_1 = E_2$ if and only if $E_1 \le E_2$ and $E_2 \le E_1$.

If for any two distinct proof values $v$ and $w$, such that $\vdash v : E$ and
$\vdash w : E$, we have that $||v|| = ||w||$, we say that $E$ is ambiguous. In a
plain manner we say that $E$ is ambiguous if any string can be parsed under $E$
in more than one way.

A regular expression consisting of nested alternations is refered to as a \emph{sum}.
All subtrees in a sum which is not itself a sum is refered to as a \emph{branch}.

\begin{example}
Let $E = (a + (b + c \times d)) + (a + b)^{*}$; we say that $E$ is a sum with
the branches $(a + (b + c \times d))$, and $(a + b)^{\ast}$.
\end{example}

\chapter{Converting XML schema to regular expression}
\label{sec:convert}

Regular expressions can be generated from XML schemas. The simples algorithm is
to generate a regular expression for the root element and then inlining the
regular expressions of the child elements

For example if a schema defines the elements \verb@A@, \verb@B@ and \verb@C@
where \verb@A@ is the root element having \verb@B@ and \verb@C@ as children in
order and \verb@C@ can appear zero or more times. Then a regular expression for
the root element would be \verb@<A></A>@ and when inlining the regular
expression for the children we get
\verb@<A><B></B>(<C></C>)*</A>@\footnote{where characters are literals and two
  characters next to each other are the concatenation of the two.}.
  
  However there are some minor problems with this simple algorithm:

  \begin{itemize}
  \item A schema can define multiple top level elements (also called global
    elements) it is impossible to know which one is intended to be the root
    element. So the actual XML file must be analysed to get the name of the root
    tag in the particular instance.

  \item Schema definitions does not mention the physical structure of the XML
    file so the regular expression needs to handle any number of whitespace
    where allowed in the XML file\footnote{For example are the following valid
      XML \texttt{<Root{\textvisiblespace}att="Foo"\textvisiblespace%
        \textvisiblespace\textbackslash{n}\textvisiblespace>\textvisiblespace%
        </Root\textvisiblespace\textvisiblespace>}}.

    This could however be solved if the original structure of the XML file was
    not guaranteed to be preserved. This way it could be stripped of any
    excessive whitespace, only leaving one space between element names and
    attributes and any whitespace in the element content

  \item 

  \end{itemize}

\subsection{Local and global schema definitions}
\label{sec:local-global-schema-definitions}
Different Schemas have different ways of defining elements, child elements and
attribute lists of elements. In general the element and child element
definitions can be divided into two principles\footnote{At least this is true for
  XSD, Relax NG schemas and properly others, but DTD only have global definitions}

\begin{description}
\item[Local definitions] is when the definition of an elements child elements is
  inlined as a child of the element definition. 

  \begin{example}[XSD schema using local definitions] \ % end the line
\begin{verbatim}
    <xsd:element name="Book"> 
      <xsd:complexType> 
        <xsd:sequence> 
          <xsd:element name="Title" type="xsd:string"/> 
           <xsd:element name="Author" type="xsd:string"/> 
        </xsd:sequence> 
      </xsd:complexType> 
    </element>
\end{verbatim}
  \end{example}

\item[Global definitions] is when the definition of an elements child elements
  is defined at toplevel, and then referenced by the element. This way multiple
  elements can share the same child element definition, but the child element is
  only defined once.

  \begin{example}[XSD schema using global definitions] \ % end the line
\begin{verbatim}
    <xsd:element name="Title" type="xsd:string"/>

    <xsd:element name="Author" type="xsd:string"/>

    <xsd:element name="Book">
      <xsd:complexType> 
        <xsd:sequence> 
          <xsd:element ref="Title"/> 
          <xsd:element ref="Author"/>
        </xsd:sequence> 
      </xsd:complexType> 
    </xsd:element>
\end{verbatim}
  \end{example}

\end{description}

And of cause any mixture of the two ways of defining elements.

\subsection{Optimising representation by global definitions}

The size of the resulting regular expression can potentially be a problem, for
large schema definitions. The problem arises especially when the data is small
as the total size is the size of the regular expression and the encoded data
combined.

Using global definitions we can just generate a regular expression once for a
child element that potentially gets references multiple times and then inline
them when referenced if the actual regular expression is needed, thus saving
space. The same apply to attributes.
\label{sec:global-definitions-save-space}

Schemas define different types of data that elements and attributes may
contain. An example is the \texttt{\#PCDATA} defined in DTD schemas that
represents parsed character data which basically is all ASCII
characters\footnote{except \&, < and > which should be encoded as entities:
  \&amp;, \&lt; and \&gt; respectively. If not \& is considered an entity
  reference and should be expanded and < and > is considered a tag and should be
  parsed.}\footnote{obviously depending on the file encoding. For simplicity we
  assume ASCII.}. Thus it is important not to expand the \texttt{\#PCDATA}
``type'' until the resulting regular expression is needed as it would expand to
a alternation of all 255 ASCII characters inside each element or attribute that
is defined as being \texttt{\#PCDATA}. Depending on what schema used simple
types should be represented by some distinct symbol that can be expanded to its
actual meaning when the resulting regular expression is needed, to save space.

\subsection{ Non regular schemas}

Most XML schema definitions allow element references (possibly to itself) which
makes it possible to create non regular languages by having
infinite

\fixme{Something more needs to be explained}

Obviously it is not possible to create regular expressions from such schema
instances.

\chapter{$\mu$-Recursion}
\label{chap:mu-recursion}

We introduce tail-recursive $\mu$-types as described in \cite{heni2010}.\\[1em]

Regular expressions offer a very restricted form of recursion namely the Kleene
star. This has an impact when we consider the bit code produced from a
particular parse tree.

To see the problem with the Kleene star from a bit coding perspective consider
the unfolding of $E^{\ast}$. It is $1 + E \times E^{\ast}$. It only takes one
bit to code the $1$. But for each Kleene star the $1$-branch is only taken
once. On the other hand each bit coding of $E$ is prefixed with one extra
bit.

If we could trade a longer code for $1$ for shorter codes for $E$ we expect to
gain higher compression rates.

Therefore we generalise recursion to tail-recursive $\mu$-types.

\section{Definition}
Let the language of expressions $UnReg_{\Sigma}^{\mu}$ over a finite alphabet
$\Sigma = \{a_1, \ldots, a_n\}$ be
\[
\alpha ::= 0 \; | \; 1 \; | \; a \; | \; \alpha_1 + \alpha_2 \; | \; \alpha_1
\times \alpha_2 \; | \; \mu X. \alpha \; | \; X
\]

The free variables of $\alpha$ is the set of variables $X$ in $\alpha$ that is
not guarded by $\mu X$. If $\alpha$ has no free variables it is closed. The term
$\alpha$ is tail-recursive if $\alpha_1$ is closed in all subexpressions of the
form $\alpha_1 \times \alpha_2$.

The language $Reg_{\Sigma}^{\mu}$ is the closed tail-recursive expressions of
$UnReg_{\Sigma}^{\mu}$.

Of course we need inhabitation rules for this new kind of expressions. But we
can reuse the rules for regular regular expressions except for the fold-rule
which now need to handle $\mu$-expressions. It becomes
\[
\inference{v : \alpha \lbrack \mu X. \alpha / X \rbrack}{\mathtt{fold}\ v : \mu X. \alpha}
\]

Theorem 9 in \cite{heni2010} states that $Reg_{\Sigma}^{\mu}$ describes exactly
the same languages as $Reg_{\Sigma}$:\fixme{Fixed the cite, check it is the right one}

\begin{enumerate}
\item $\forall E \in Reg_{\Sigma} : \exists \alpha \in Reg_{\Sigma}^{\mu} :
  \{||v|| | |- v : E\} = \{||v|| | |- v : \alpha \}$
\item $\forall \alpha \in Reg_{\Sigma}^{\mu} : \exists E \in Reg_{\Sigma} :
  \{||v|| | |- v : \alpha\} = \{||v|| | |- v : E \}$
\end{enumerate}

And the proof of $1$ tells us how to convert a regular expression into an
equivalent one using $\mu$-recursion: Replace every $E^{\ast}$ with $\mu X.1 +
\alpha \times X$ where $\alpha$ is the conversion of $E$.

\section{Bit coding}

We extend the rules for producing bit codes to closed tail-recursive
$\mu$-expressions. They are given in \fref[plain]{fig:code} and
\fref{fig:decode}. Notice that there a no rules for $X$ as $Reg_{\Sigma}^{\mu}$
only contains closed expressions.

\begin{figure}
\[
\begin{array}{lcl}
\mathtt{code}(\mathrm{()} : 1) & = & \epsilon \\
\mathtt{code}(a : a) & = & \epsilon \\
\mathtt{code}(\mathrm{inl} \, v : \alpha_1 + \alpha_2) & = & 0 \; \mathtt{code}(v : \alpha_1) \\
\mathtt{code}(\mathrm{inr} \, v : \alpha_1 + \alpha_2) & = & 1 \; \mathtt{code}(v : \alpha_2) \\
\mathtt{code}((v,w) : \alpha_1 \times \alpha_2) & = & \mathtt{code}(v : \alpha_1) \; \mathtt{code}(v : \alpha_2) \\
%\mathtt{code}(\mathrm{fold} \, v : E^{*}) & = & \mathtt{code}(v : 1 + E \times E^{*}) \\
%\mathtt{code}(v : X) & = & \mathtt{code}(v : )
\mathtt{code}(\mathrm{fold} \, v : \mu X . \alpha) & = & \mathtt{code}(v : \alpha[\mu X . \alpha / X])
\end{array}
\]
\caption{Type directed encoding from proof values to bit sequences.}
\label{fig:code}
\end{figure}

\begin{figure}
\[
\begin{array}{lcl}
\mathtt{decode'}(d, 1) & = & ((), d) \\
\mathtt{decode'}(d, a) & = & (a, d) \\
\mathtt{decode'}(0d, E + F) & = & \mathbf{let} \; (v,d') = \mathtt{decode'}(d,E) \; \mathbf{in} \; (\mathrm{inl} \, v, d') \\
\mathtt{decode'}(1d, E + F) & = & \mathbf{let} \; (w,d') = \mathtt{decode'}(d,F) \; \mathbf{in} \; (\mathrm{inl} \, w, d') \\
\mathtt{decode'}(d, E \times F) & = &
    \mathbf{let} \left\{ \begin{array}{rcl} (v,d') &=& \mathtt{decode}'(d, E) \\ (w, d'') &=& \mathtt{decode}'(d', F) \end{array} \right\}
    \mathbf{in} \; ((v,w), d'') \\
%\mathtt{decode'}(d, E^{*}) & = & \mathbf{let} \; (v,d') = \mathtt{decode'}(d : 1 + E \times E^{*}) \; \mathbf{in} \; (\mathrm{fold} \, v, d')\\
\mathtt{decode'}(d, \mu X . \alpha) = \mathbf{let} \; (v,d') & = & \mathtt{decode'}(d, \alpha[\mu X . \alpha / X]) \; \mathbf{in} \; (\mathrm{fold} \, v, d') \\
\\
\mathtt{decode}(d, E) & = & \mathbf{let} \; (v, d') = \mathtt{decode'}(d, E) \; \mathbf{in} \; v
\end{array}
\]
\caption{Type directed decoding from bit sequences to proof values.}
\label{fig:decode}
\end{figure}

\section{Normalising}
One gains nothing just by converting a regular expression to an equivalent
$\mu$-recursive one by the rule given above. Consider an example: bit code the
string $aab$ under the regular expression $(a + (b + c))^{\ast}$. The bit codes
for $a$ and $b$ under $(a + (b + c))$ being $0$ and $10$ respectively we get
$10101100$.

Now bit code the same string under the equivalent $\mu X. 1 + (a + (b + c))
\times X$. Again the result is $10101100$.

But observe that $\mu X. 1 + (a + (b + c)) \times X = \mu X. a \times X + (b
\times X + (c \times X + 1))$. Bit coding $aab$ with regard to the latter yields
the code $0010$. That is a reduction of 50\%.

The reason why the bit code under the latter regular expression is shorter is
that the $1$ is buried inside the sum. That way we don't use a full bit per
character to code that the string is not finished yet.

It is easy to automatically balance a sum (see \fref{chap:huffman}) but that
doesn't immediately help as the ``$\times X$'' prevents us from including the
$1$ in the balancing.

Luckily we can exploit the distributivity of $\times$ with respect to $+$. We
rewrite $\mu$-recursive expressions to their normal form using the system of
inference rules given in \fref{fig:normal-form}. We write $|- \alpha ->n
\alpha'$ if $\alpha ->n \alpha'$ is derivable in the system.

\begin{figure}
\[
dist_1 : \inference{\gamma \times \alpha + \gamma \times \beta ->n \delta}{\gamma \times
  (\alpha + \beta) ->n \delta} \qquad
dist_2 : \inference{\alpha \times \gamma + \beta \times \gamma ->n \delta}{(\alpha +
  \beta) \times \gamma ->n \delta}(\gamma \neq \gamma_1 + \gamma_2)
\]

\[
sum : \inference{\alpha ->n \alpha' & \beta ->n \beta'}{\alpha + \beta ->n \alpha' +
  \beta'} \qquad
rec : \inference{\alpha ->n \alpha'}{\mu X. \alpha ->n \mu X. \alpha'}
\]

\[
prod : \inference{\alpha ->n \alpha' & \beta ->n \beta'}{\alpha \times \beta ->n \alpha'
  \times \beta'}(\alpha \neq \alpha_1 + \alpha_2 \land \beta \neq \beta_1 + \beta_2)
\]

\[
sym : \inference{}{a ->n a} \qquad
one : \inference{}{1 ->n 1} \qquad
zero : \inference{}{0 ->n 0} \qquad
var : \inference{}{X ->n X}
\]
\label{fig:normal-form}
\end{figure}

Informally normal forms have the following properties which we show below.
\begin{enumerate}
\item The language $Reg_{\Sigma}^{\mu}$ is closed under normal form conversion.
\item The normal form conversion gives way to a total function $norm :
  Reg_{\Sigma}^{\mu} -> Reg_{\Sigma}^{\mu}$.
\item An expression is equivalent to it's normal form. They describe the same
  language.
\item The bit code size does not blow up under normal forms.
\end{enumerate}

\begin{theorem}
  For all $\alpha \in Reg_{\Sigma}^{\mu}$ and $\alpha'$ such that $|- \alpha ->n
  \alpha'$ we have that $\alpha' \in Reg_{\Sigma}^{\mu}$.

  \begin{proof}
    It is immediately clear that $\alpha' \in UnReg_{\Sigma}^{\mu}$. So we need
    to show that $\alpha'$ is closed and tail-recursive.

    To see that it is closed notice that the rules never moves anything past a
    $\mu X$ and that variables are not added or renamed. This property does not
    rely on tail-recursiveness so we can use it as a lemma in itself.

    To show that $\alpha'$ is tail-recursive we need to show that $\alpha'_1$ is
    closed in all subexpressions of the form $\alpha'_1 \times \alpha'_2$.
    For each such subexpressions there must be a subderivation in the derivation(s)
    of $\alpha ->n \alpha'$ of the form
    \[
    \inference{\mathcal{E} & \mathcal{F} \\ \beta ->n \beta' & \gamma ->n
      \gamma'}{\beta \times \gamma ->n \beta' \times \gamma'}
    \]
    From the derivations $\mathcal{E}$ and $\mathcal{F}$ we have $|- \beta ->n
    \beta'$ and $|- \gamma ->n \gamma'$ respectively. We also know from the
    tail-recursiveness of $\alpha$ that $\beta$ is closed. So by the lemma
    stated above we must have that $\beta'$ is also closed which in turn means
    that $\alpha'$ is tail-recursive.
  \end{proof}
\end{theorem}

\begin{theorem}
  Define $norm(\alpha) = \alpha'$ where $|- \alpha ->n \alpha'$. Then $norm :
  Reg_{\Sigma}^{\mu} -> Reg_{\Sigma}^{\mu}$ is indeed well-defined and total.

  \begin{proof}
    We need to show three things about the rules for $->n$:
    \begin{enumerate}
    \item Termination. That is there are no infinite derivations.
    \item Completeness. For every $\alpha$ there exists a derivation $\alpha ->n
      \alpha'$.
    \item Determinism. For every $\alpha$ there exists at most one derivation
      $\alpha ->n \alpha'$.
    \end{enumerate}
    \subsubsection{Termination}
    The only rules whose premises are not strictly smaller than the conclusion
    are $dist_1$ and $dist_2$. So we need to show that these rules cannot result
    in an infinite derivation. They are similar so we only show it for
    $dist_1$. A subderivation $\mathcal{E}$ ending in $dist_1$ must have the
    form
    \[
    \inference{\inference{\mathcal{F} & \mathcal{G} \\ \gamma \times \alpha ->n
        \delta_1 & \gamma \times \beta ->n \delta_2 }{\gamma \times \alpha +
        \gamma \times \beta ->n \delta_1 + \delta_2}}{\gamma \times (\alpha +
      \beta) ->n \delta}
    \]
    The conclusions of $\mathcal{F}$ and $\mathcal{G}$ are smaller than the
    conclusion of $\mathcal{E}$.

    Then the result follows by structural induction.

    \subsubsection{Completeness}
    Comparing the language of $UnReg_{\Sigma}^{\mu}$ to the rules and noting
    that $Reg_{\Sigma}^{\mu} \subset UnReg_{\Sigma}^{\mu}$ we see that the rules
    are complete.

    \subsubsection{Determinism}
    The rules $dist_1$, $dist_2$ and $prod$ all end in $\times$ but the
    side-conditions ensure that only one of them is applicable at any time. So
    the rules are deterministic.

    This concludes our proof.
  \end{proof}
\end{theorem}

\begin{theorem}
  If $norm(\alpha) = \alpha'$ then $\mathcal{L} |[ \alpha |] = \mathcal{L} |[
  \alpha' |]$.

  \begin{proof} (Sketch).
    Since the inference rules for normal form conversion are defined for
    possibly non-closed expressions we need to prove a stronger property. Namely
    for all closed expressions $\alpha_i$ and derivations $\alpha ->n \alpha'$
    then $\alpha \lbrack \alpha_i / X_i \rbrack = \alpha' \lbrack \alpha'_i /
    X_i \rbrack$ where the $X_i$'s are the free variables of $\alpha$ and
    $\alpha_i ->n \alpha'_i$ is derivable. It is easy to see that the free
    variables of $\alpha$ and $\alpha'$ are the same.

    To prove the theorem we would need a system for proving containment among
    tail-recursive closed $\mu$-expressions. Alas we do not have such a system
    but we speculate that the system of \cite{heni2010} is still sound and
    complete with the additional rule\fixme{fixed the cite, is it correct?}
    \[
    rec : \mu X. \alpha = \alpha \lbrack \mu X. \alpha / X \rbrack
    \]
    Noticing that the free variables of $\mu X. \alpha$ is a subset of the free
    variables of $\alpha$ the result follows by structural induction on the
    expression.
  \end{proof}
\end{theorem}

\begin{theorem}
  Conversion to normal form preserves bit coding size.

  Since regular expressions are, in general, ambiguous the property we are after
  here is not so obvious; Again we need to handle possible non-closed
  expressions. Given an expression $\alpha$ let $\bar{\alpha} = \alpha \lbrack
  \alpha_i / X_i \rbrack$, $\bar{\alpha}' = \alpha' \lbrack \alpha'_i / X_i
  \rbrack$ where the $X_i$'s are the free variables of $\alpha$, the
  $\alpha_i$'s are arbitrary closed expressions, and $\alpha ->n \alpha'$ and
  $\alpha_i ->n \alpha'_i$ are derivable.

  Now if $v$ is a proof of membership in $\bar{\alpha}$ with the bit coding
  $\mathfrak{b}$ then there exists a membership proof $v'$ in $\bar{\alpha}'$
  with the bit coding $\mathfrak{b'}$ such that $| \mathfrak{b'} | \leq |
  \mathfrak{b} |$.

  \begin{proof}
    With the problem properly formulated the proof itself is almost trivial as
    is often the case.

    Again we use structural induction on $\alpha$. The interesting cases are
    $dist_1$, $dist_2$ and $rec$. The cases for $dist_1$ and $dist_2$ are
    analogous so we give here only the first one.\\[1em]


    Case $dist_1$: By the induction hypothesis we have that if the bit coding of
    $w : (\gamma \times \alpha + \gamma \times \beta)\lbrack \alpha_i / X_i
    \rbrack$ is $\mathfrak{c}$ then $| \mathfrak{b'} | \leq | \mathfrak{c}
    |$. Now we make a case distinction on the first bit $\mathfrak{c_0}$ of
    $\mathfrak{c}$. In either case the first part of the remaining bits
    $\mathfrak{c_1}$ codes a membership proof of $\gamma\lbrack \alpha_i / X_i
    \rbrack$. If $\mathfrak{c_0}$ is $0$ then the rest of the bits
    $\mathfrak{c_2}$ code at membership proof of $\alpha\lbrack \alpha_i / X_i
    \rbrack$, and if it is $1$ then $\mathfrak{c_2}$ codes a membership proof of
    $\beta\lbrack \alpha_i / X_i \rbrack$. In either case we can reorder the bit
    code to construct one that codes a membership proof of $(\gamma \times
    (\alpha + \beta))\lbrack \alpha_i / X_i \rbrack$, namely
    $\mathfrak{c'} = \mathfrak{c_1}\mathfrak{c_0}\mathfrak{c_2}$.\\[1em]

    Case $rec$: The
    proof follows by noticing that the free variables of $\mu X. \alpha$ is a
    subset of the free variables of $\alpha$.
  \end{proof}

  Alternation is the sole reason for ambiguity, so a parser can be made
  unambiguous by always choosing left over right or vice versa.

  It is not too hard to see that the bit coding size is preserved exactly
  (eg. $| \mathfrak{b'} | = | \mathfrak{b} |$) when using such a parser.
\end{theorem}

\chapter{Specializing regular expressions to strings using Huffman trees}
\label{chap:huffman}
\label{chap:specialize_huffman}
\fixme{two labels in latex code.. :S}
 \fixme{Chapter name is kind of to loong?}

The fact that the alternation operator in $Reg_{\Sigma}$ is associative and
commutative with respect to language equivalence can be exploited for encoding
strings more efficiently. Consider the string \texttt{"aaa"}; compressing it
using the expression $E_1 = ((a + b) + c)^{*}$ yields the bit sequence
$111111110$. On the other hand, compressing it using the expression $E_2 = (a +
(b + c))^{*}$ yields the bit sequence $111110$. As $\mathcal{L}(E_1) =
\mathcal{L}(E_2)$ both expressions can be used for compressing the exact same
set of strings; for the specific string \texttt{"aaa"} we prefer $E_2$. For an
arbitrary string, $s$, and an arbitrary regular expression, $E$, where $s \in
\mathcal{L}(E)$, we find that the optimal method for reordering the sums in $E$,
with respect to the compression ratio, is to reorder them as Huffman trees based
on $s$.

To do that, we introduce the syntactic form $Reg'_\Sigma$ over the finite
alphabet $\Sigma = {a_1, \dots, a_n}$:

\[
    E ::= 0 \, | \, 1 \, | \, a \, | \, \Sigma(E_1, E_2, E_3, \dots, E_n) \, | \, E_1 \times E_2 \, | \, E^{*}
\]

The operator $\Sigma(E_1, E_2, E_3, \dots, E_n)$ denotes a sum of alternations,
and the rest of the symbols denotes the same as in $Reg$. As $Reg'$ will only be
used as a temporary syntactic form for reordering the sums, the order of
association of the alternations inside a $\Sigma$ is insignificant.  Next we
introduce what we choose to call the \emph{flattened form} of an expression $E$;
to bring $E$ into the \emph{flattened form}, $E'$, we flatten out all nested
sums in $E$ and rewrite them in $Reg'$ using the sum operator $\Sigma$; i.e. $(a
+ (b \times c + d) + e \times f)^{*} \times (c + d)$ will be rewritten into
$\Sigma(a, b \times c, d, e \times f)^{*} \times \Sigma{(c, d)}$.

After having flattened out an expression $E$ into $E'$, we pick a parse tree
$p_s$ given the input string $s$, such that $p_s : E$ and $||p_s|| = s$; by
running trough the parse tree, we can for each sum $\Sigma(E_1, \dots, E_n)$ in
$E'$ count how many times $E_k \in 1 \dots n$ is selected in the parse tree.

Finally, we can for each sum $\Sigma$ in $E'$ use these countings to reorder the
$\Sigma$ back into the syntactic form of $Reg$, by ordering the alternations as
Huffman trees.

\begin{example}
  Consider the string $s = \mathtt{"abbc"}$ and the regular expression $E = (a +
  ((b + c) + d))^{*}$. Parsing $s$ using $E$ yields the following parse tree:
\[
\begin{array}{rcl}
p_s & = & \mathtt{fold}(\mathtt{inr}(\mathtt{inl} \; a, \\
    &   & \mathtt{fold}(\mathtt{inr}(\mathtt{inr}(\mathtt{inl}(\mathtt{inl} \; b), \\
    &   & \mathtt{fold}(\mathtt{inr}(\mathtt{inr}(\mathtt{inl}(\mathtt{inl} \; b), \\
    &   & \mathtt{fold}(\mathtt{inr}(\mathtt{inr}(\mathtt{inl}(\mathtt{inr} \; c), \\
    &   & \mathtt{fold}(\mathtt{inl}(()))))))))))))))
\end{array}
\]
\noindent Encoding $s$ using $e$ gives the bit sequence $101100110011010$ (15
bits). Now when translating $E$ into $Reg'$ we get the expression $E' =
\Sigma{(a, b, c, d)}^{*}$. Counting the number of times each path in the sum
$\Sigma{(a, b, c, d)}$ is taken in $p_s$ we get:

\begin{center}
\begin{tabular}{c|c|c|c}
$a$ & $b$ & $c$ & $d$ \\
\hline
1   & 2   & 1   & 0
\end{tabular}
\end{center}

\noindent Finally by reordering the sum into an Huffman tree using the above
frequencies we get the $Reg$-expression $F = (b + (a + (c + d)))^{*}$. Encoding
$s$ using $F$ gives the bit sequence $110101011100$ (12 bits); thus we have
spared 3 bits.

\end{example}

\chapter{Implementation of DTD schema parser}

We have implemented a DTD schema parser, even though it limits the possibilities
of optimisation by restricting specific types on elements contained data (e.g.,
only integers or only dates).

Our decision to do this anyway was that the DBLP\footnote{The DBLP Computer
  Science Bibliography \url{http://www.informatik.uni-trier.de/~ley/db/}}
database comes with a DTD
schema\footnote{\url{http://www.informatik.uni-trier.de/~ley/db/about/dblp.dtd}}
ready to use. This is handy as the DBLP database can be fetched in at least 4
versions: a up to date, 2009, 2008, 2006 and 2004 version, each getting bigger
up until the current up to date version.

We utilise the HaXmL Haskell package to parse the DTD file and then we transform
this into an extended regular expression data type. Currently we only support as
much as the DTD syntax that was needed to parse the DBLP's DTD schema which at
least includes most of elements (element content and mixed content) and
attribute definitions.

The extended regular expression adds query (\texttt{?}: zero or more), plus
(\texttt{+}: one or more) and some character classes, especially one for
representing the \texttt{\#PCDATA} type. This extended regular expression data
type is intended as being the high level representation used when saving the
regular expression with the encoded data so it is as small as possible.


\chapter{Experimental results}
\label{sec:experimental_results}

file: 33414 bytes (data: 17927, meta: 15171)

gzip file: 5760 bytes

gzip data: 5000 bytes

code meta: 737 bytes

gzip data + code meta: 5737 bytes

\chapter{Further work}

Below is a list of possible further work.
\fixme{Should be merged with real further work}

\begin{description}
\item[Refactoring] As it is not all schema instances that utilises global
  definitions (see \label{sec:local-global-schema-definitions}), it would make
  sense to extract such child elements and attributes into being global
  declarations instead as long as the result is not bigger than the
  original\footnote{We need to remember that some sort of reference needs to be inserted
    which also takes some space.}.


\item A better compression ratio can properly be gained by preprocessing the XML
  file and when any data (not markup) is found, strip at away and put it into a
  buffer, leaving the index where the data was in the file. This way we would
  end up with a XML file that contains lots of markup which we encode using bit
  encoding, and then encode all the collected data with some arbitrary
  statistically compression method (e.g., the Lempel–Ziv–Welch (LZW) algorithm)

  As statistically compression algorithms works better when the same type of
  data is grouped together, then it would properly be a good idea to group the
  values of all elements and attributes that have the same name. For example all
  data from elements named email would then be grouped together.

\end{description}

\chapter{Conclusion}
\label{sec:conclusion}

\section{Future work}

\fixme{add something here}

\clearpage

\bibliographystyle{../bibliography/theseurl}
\bibliography{../bibliography/bibliography}

\end{document}

%%% Local Variables: 
%%% mode: tex
%%% TeX-master: t
%%% End:
