% http://web.science.mq.edu.au/~rdale/resources/writingnotes/latexstyle.html

\documentclass[a4paper, oneside]{memoir}
% Fixes "No room for a new \xxx" error by extending the default 256 fixed size
% LaTeX arrays
\usepackage{etex}
%\reserveinserts{28}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[british]{babel}

% bedre orddeling Gør at der som minimum skal blive to tegn på linien ved
% orddeling og minimum flyttes to tegn ned på næste linie. Desværre er værdien
% anvendt af babel »12«, hvilket kan give orddelingen »h-vor«.
\renewcommand{\britishhyphenmins}{22}

% Fix of fancyref to work with memoir. Makes references look
% nice. Redefines memoir \fref and \Fref to \refer and \Refer.
% \usepackage{refer} %
% As we dont really have any use for \fref and \Fref we just undefine what
% memoir defined them as, so fancyref can define what it wants.
\let\fref\undefined
\let\Fref\undefined
\usepackage{fancyref} % Better reference.

\usepackage{pdflscape} % Gør landscape-environmentet tilgængeligt
\usepackage[draft]{fixme}     % Indsæt "fixme" noter i drafts.
\usepackage{hyperref}  % Indsæter links (interne og eksterne) i PDF

\usepackage{mdwtab}
\usepackage{mathenv}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{semantic} % for the \mathlig function

\usepackage[format=hang]{caption,subfig}
\usepackage{graphicx}
\usepackage{stmaryrd}
\usepackage[final]{listings} % Make sure we show the listing even though we are
                             % making a final report.

\usepackage{ulem} % \sout - strike-through
% ulem changes \emph to be underline instead of being italic.
% So we change it back to be italic.
\normalem

\usepackage{tikz}

\lstset{ %
% language=Octave,                % choose the language of the code
basicstyle=\ttfamily,        % the size of the fonts that are used for the code
basewidth=0.5em,
% numbers=left,                   % where to put the line-numbers
% numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
% stepnumber=2,                   % the step between two line-numbers. If it's 1 each line will be numbered
% numbersep=5pt,                  % how far the line-numbers are from the code
% backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
% showspaces=false,               % show spaces adding particular underscores
% showstringspaces=false,         % underline spaces within strings
% showtabs=false,                 % show tabs within strings adding particular underscores
% frame=single	                % adds a frame around the code
% tabsize=2,	                % sets default tabsize to 2 spaces
% captionpos=b,                   % sets the caption-position to bottom
% breaklines=true,                % sets automatic line breaking
% breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
escapeinside={(@}{@)}          % if you want to add a comment within your code
}

\renewcommand{\ttdefault}{txtt} % Bedre typewriter font
% \usepackage[sc]{mathpazo}     % Palatino font
\renewcommand{\rmdefault}{ugm} % Garamond
%\usepackage[garamond]{mathdesign}

%\overfullrule=5pt
%\setsecnumdepth{part}
\setcounter{secnumdepth}{1} % Sæt overskriftsnummereringsdybde. Disable = -1.

% fix to make this look like an article.. Really ugly but we have a 10page limit!!!!!
\chapterstyle{article} % changes style of chapters, to look nice.

\let\clearforchapter\par % Dont clear the page before each chapter. This saves
                         % pages.. 10 page limit !!!!

\makeatletter
\newenvironment{nonfloatingfigure}{
  \vskip\intextsep
  \def\@captype{figure}
  }{
  \vskip\intextsep
}

\newenvironment{nonfloatingtable}{
  \vskip\intextsep
  \def\@captype{table}
  }{
  \vskip\intextsep
}
\makeatother


\theoremstyle{definition}
\newtheorem{judgment}{Judgment}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}

\newcommand*{\fancyrefdeflabelprefix}{def}
\fancyrefaddcaptions{english}{
  \newcommand*{\Frefdefname}{Definition}
  \newcommand*{\frefdefname}{\MakeLowercase{\Frefdefname}}
}
\frefformat{vario}{\fancyrefdeflabelprefix}{%
  \frefdefname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyrefdeflabelprefix}{%
  \Frefdefname\fancyrefdefaultspacing#1#3%
}

\newcommand*{\fancyreflemlabelprefix}{lem}
\fancyrefaddcaptions{english}{
  \newcommand*{\Freflemname}{Lemma}
  \newcommand*{\freflemname}{\MakeLowercase{\Freflemname}}
}
\frefformat{vario}{\fancyreflemlabelprefix}{%
  \freflemname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyreflemlabelprefix}{%
  \Freflemname\fancyrefdefaultspacing#1#3%
}
\frefformat{plain}{\fancyreflemlabelprefix}{%
  \freflemname\fancyrefdefaultspacing#1%
}
\Frefformat{plain}{\fancyreflemlabelprefix}{%
  \Freflemname\fancyrefdefaultspacing#1%
}

\newcommand*{\fancyrefproplabelprefix}{prop}
\fancyrefaddcaptions{english}{
  \newcommand*{\Frefpropname}{Proposition}
  \newcommand*{\frefpropname}{\MakeLowercase{\Frefpropname}}
}
\frefformat{vario}{\fancyreflemlabelprefix}{%
  \freflemname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyreflemlabelprefix}{%
  \Freflemname\fancyrefdefaultspacing#1#3%
}
\frefformat{plain}{\fancyreflemlabelprefix}{%
  \freflemname\fancyrefdefaultspacing#1%
}
\Frefformat{plain}{\fancyreflemlabelprefix}{%
  \Freflemname\fancyrefdefaultspacing#1%
}

\newcommand*{\fancyrefthmlabelprefix}{thm}
\fancyrefaddcaptions{english}{
  \newcommand*{\Frefthmname}{Theorem}
  \newcommand*{\frefthmname}{\MakeLowercase{\Frefthmname}}
}
\frefformat{vario}{\fancyrefthmlabelprefix}{%
  \frefthmname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyrefthmlabelprefix}{%
  \Frefthmname\fancyrefdefaultspacing#1#3%
}

\newcommand*{\fancyrefcorlabelprefix}{cor}
\fancyrefaddcaptions{english}{
  \newcommand*{\Frefcorname}{Corollary}
  \newcommand*{\frefcorname}{\MakeLowercase{\Frefcorname}}
}
\frefformat{vario}{\fancyrefcorlabelprefix}{%
  \frefcorname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyrefcorlabelprefix}{%
  \Frefcorname\fancyrefdefaultspacing#1#3%
}

\newcommand*{\fancyrefexlabelprefix}{ex}
\fancyrefaddcaptions{english}{
  \newcommand*{\Frefexname}{Example}
  \newcommand*{\frefexname}{\MakeLowercase{\Frefexname}}
}
\frefformat{vario}{\fancyrefexlabelprefix}{%
  \frefexname\fancyrefdefaultspacing#1#3%
}
\Frefformat{vario}{\fancyrefexlabelprefix}{%
  \Frefexname\fancyrefdefaultspacing#1#3%
}
\frefformat{plain}{\fancyrefexlabelprefix}{%
  \frefexname\fancyrefdefaultspacing#1%
}
\Frefformat{plain}{\fancyrefexlabelprefix}{%
  \Frefexname\fancyrefdefaultspacing#1%
}

\newcommand{\ttt}[1]{\texttt{#1}}
\newcommand{\tnm}[1]{\textnormal{#1}}
\newcommand{\mrm}[1]{\mathrm{#1}}

\newcommand{\Cov}{\mathrm{Cov}}
\providecommand{\FV}{\mathrm{FV}}
\providecommand{\Dom}{\mathrm{Dom}}

\mathlig{||}{\Vert}
\mathlig{<'}{\prec}
\mathlig{>'}{\succ}
\mathlig{<='}{\preccurlyeq}
\mathlig{>='}{\succcurlyeq}
\mathlig{<=}{\leqslant}
\mathlig{>=}{\geqslant}
\mathlig{<>}{\neq}
\mathlig{|=}{\sqsubset}
\mathlig{=|}{\sqsupset}
\mathlig{==}{\equiv}
\mathlig{==a}{=_{\alpha}}
\mathlig{<|}{\lhd}
\mathlig{|>}{\rhd}
\mathlig{++}{\mathrel{\mbox{+\!\!\!+}}}
% ~>e or ~>g conflicts with the \cite command for some reason.
\mathlig{->e}{\stackrel{elim}{\leadsto}}
\mathlig{->g}{\stackrel{gen}{\leadsto}}
\mathlig{->n}{\leadsto}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	    	     Forside
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\makeatletter % open mode for reading @ signed variables 
\def\maketitle{%
  \null
  \thispagestyle{empty}%
  \vfill
  \begin{center}\leavevmode
    % \normalfont
    % \Huge{\raggedleft \@title\par}%
    % \hrulefill\par
    % \Large{\raggedright \subtitle\par}%
    \begin{flushleft}
      \huge \@title
    \end{flushleft}
    \par
    \hrule height 1pt
    \par
    \begin{flushright}
      \large \subtitle \par
      \vspace{2pt}
      \footnotesize DIKU \@date
    \end{flushright}
    \vskip 10em
    \begin{abstract}
      \textit{\descript}
    \end{abstract}
  \end{center}%
  \vfill
\begin{minipage}{80pt}
\includegraphics*[scale=0.75]{imgs/nat-logo}
\end{minipage}
\begin{minipage}{300pt}
  \begin{flushleft}
    {\large \@author } \\
    {\footnotesize \suplementInfo }

  \end{flushleft}
\end{minipage}
\cleardoublepage % lave 1 ekstre side blank efter
  \clearpage % Terminates the page here. Everything else vil be placed on next page.
}
\makeatother % closing mode for reading @ signed variables
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		Data til forside
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Regular-expression based bit coding}
\def\subtitle{TiPL - Topics in Programming Languages}
\author{Morten Brøns-Pedersen {\footnotesize (mortenbp@gmail.com)}\\
Jesper Reenberg {\footnotesize (jesper.reenberg@gmail.com)} \\
Nis Wegmann {\footnotesize (niswegmann@gmail.com)}}
\def\descript
{
  We investigate a method for compressing data using regular expressions: First
  the data is parsed under a regular expression matching the data. Second the
  parse tree is bit coded efficiently using an oracle based approach.

  We show how to deploy the method on XML files. By automatically deriving a
  regular expression from an XML scheme, we can compress all XML instances of
  that scheme.

  Based on our method we have implemented a prototype of an XML compressor in
  the functional language Haskell. Further we have tested the prototype on data
  from the Digital Bibliography \& Library Project; we are able to compress the
  data by a factor of more than four.
}

\def\suplementInfo{
  \kern 5pt \hrule width 11pc \kern 5pt % putter 5pt spacing oven over og neden under stregen
  Dept. of Computer Science \\
  University of Copenhagen}
% \date{} % used to set explicit dates

\pagestyle{plain}

\begin{document}

\frontmatter

\maketitle
\thispagestyle{empty}

\clearpage

\tableofcontents*

\mainmatter

%\url{http://dictionary.cambridge.org/dictionary/british/refer-sb-to-sth_2#refer-sb-to-sth_2__2}

\chapter{Introduction}
Extensible Markup Language (XML) \cite{w3c2000} is an extensively used format
for representing and sharing data on the web. During the last decade hundreds
of XML-based domain specific languages has emerged, e.g. RSS for publishing web
blogs, XHTML for encoding web pages, CML for managing molecular information,
SVG for describing two-dimensional vector graphics, and OpenDocument for
representing electronic documents. Further, XML has become ubiquitous in both commercial
industries such as banking and telecommunication \cite{lisu2000}, and in science
and research industries \cite{zhlu2002, gi2000}.

The advantage of using XML is flexibility and access to the vast amount of
existing tools and technologies tailored at XML. Nonetheless encoding data
in XML is highly inefficient in terms of space-usage, as large amounts of
meta-data are replicated throughout the encoded file; hence tools for compressing
XML encoded data is of main concern.

Existing general data-compressing techniques are typically based on a
statistical approach using codebooks. In this paper we take another approach and
investigate a method specific for XML files which exploits the tidy semantics of
the XML format.  Given an XML scheme file \cite{wa2001}, we automatically derive
a regular expression which matches all instances of the XML scheme. Using this
regular expression enables us to parse an instance of the XML scheme, and we
encode the parse tree efficiently using a bit coding technique which only encode
non-deterministic parts of the parse tree. We have implemented a prototype of
our method in Haskell \cite{sugost2008} and conducted experiments using this
prototype for compressing data obtained from the Digital Bibliography \& Library
Project .

This paper marks the conclusion of the 2010 candidate course \emph{Topics in
  Programming Languages} at Department of Computer Science, Copenhagen
University.

\section{Prerequisites}

We expect readers to posses basic knowledge of regular expressions
\cite{houl2006}.  Also we expect familiarity with inference system and
programming language semantics \cite{gu1992, wi1993}.  Knowledge of XML and DTD
schemes is necessary to understand \fref{chap:convert}, and knowledge of Huffman
coding \cite[section 16.3]{co2001} is necessary to understand
\fref{chap:huffman}.

\section{Previous work}

Our work is based on Henglein and Nielsen's paper \cite{heni2010}. In their
paper they propose a straightforward technique for encoding syntax trees parsed
under regular expressions efficiently as bit sequences. This is related to the
work done by Necula and Rahul \cite{nera2001} where a variant of the same
technique is used for compact encoding of proof-carrying code. They find that
it is much more space-efficient to encode proofs as oracles. Given an oracle the
proof can then be reconstructed by a non-deterministic theorem prover.

In \cite{lisu2000} Leifke and Suciu describes their XML compressor
\emph{XMill}. By using a specialized version of \emph{zlib} \cite{dega1996} they
are able to compress XML files up to twice of the compression ratio of GZIP
\cite{de1996}.

\section{Outline}

The rest of the paper is organized as follows:
\Fref{chap:formalisation} lays down the formal foundation on which we reason about regular expressions;
\Fref{chap:convert} shows how to automatically derive a regular expression from a DTD scheme;
\Fref{chap:mu-recursion} extends regular expressions with recursion, and shows how to encode parse trees efficienctly;
\Fref{chap:huffman} shows how to specialize a regular expression to the specific XML-data being compressed;
\Fref{chap:experimental_results} evaluates our prototype and presents experimental results;
\Fref{chap:conclusion} concludes the paper and presents further work.

\section{Acknowledgement}

The authors extend their gratitude to Joakim Ahnfelt-Rønne, Jon Elverkilde, and
Michael Flænø Werk for insightful comments. Also we would like to thank Lasse
Nielsen for providing useful source code for regular expression parsing.

\chapter{Formalisation}
\label{chap:formalisation}

\begin{figure}
\[
\begin{array}{ccc}
  \inference{}{() : 1}
&
  \inference{}{a : a}
&
  \inference{v : E}{\mathtt{inl}\ v : E + F}
\\
\\
  \inference{v : F}{\mathtt{inr}\ v : E + F}
&
  \inference{v : E & w : F}{(v, w) : E \times F}
&
  \inference{v : 1 + E \times E^{\ast}}{\mathtt{fold}\ v : E^{\ast}}
\end{array}
\]
\caption{Inhabitation rules for regular expressions.}
\label{fig:inhabitation}
\end{figure}

We define the language of regular expressions $Reg_\Sigma$ over the finite
alphabet $\Sigma = {a_1, \dots, a_n}$ as follows:

\[
    E ::= 0 \; | \; 1 \; | \; E_1 + E_2 \; | \; E_1 \times E_2 \; | \; E^{*}
\]

\noindent with ${}^{\ast}$ having the highest precedence, and $\times$ having
higher precedence than $+$.

Based on the formalisation of Henglein and Nielsen \cite{heni2010} we focus on
string parsing under regular expressions. \Fref{fig:inhabitation}
shows their inference system, in which inhabitation proofs for regular
expressions can be derived. We write $\vdash v : E$ if $v : E$ is derivable in
it. If $|- v : E$ we refer to $v$ as a \emph{proof value} or a \emph{parse tree}.

The \emph{flattening} $||v||$ of a proof value $v$ is defined in \fref{fig:flatten}.

\begin{figure}
\[
\begin{array}{rclrcl}
||()|| & = & \epsilon &
||a||  & = & a \\
|| \mathtt{inl} \; v|| & = & ||v|| &
|| \mathtt{inr} \; v|| & = & ||v|| \\
|| (v,w)|| & = & ||v||||w|| &
|| \mathtt{fold} \; v|| & = & ||v||
\end{array}
\]
\label{fig:flatten}
\end{figure}

The \emph{language} $\mathcal{L} |[E|]$ of the regular expression $E$ is defined by

\[
\mathcal{L}|[E|] = \lbrace ||v|| \; | \; \vdash v : E \rbrace.
\]

Equivalence of regular expression $E_1$ and $E_2$ is denoted by $E_1 = E_2$ and
holds iff $\mathcal{L}|[E_1|] = \mathcal{L}|[E_2|]$; Containment of regular
expression $E_1$ in $E_2$ is denoted by $E_1 \le E_2$ and holds if
$\mathcal{L}|[E_1|] \subseteq \mathcal{L}|[E_2|]$.  It follows from basic set
theory that $E_1 = E_2$ if and only if $E_1 \le E_2$ and $E_2 \le E_1$.

There are several (sound and complete) proof systems for equivalence among
regular expressions given in the literature (\cite{salomaa1966, kozen1994,
  grabmayer2005, heni2010}).

If for any two distinct proof values $v$ and $w$, such that $\vdash v : E$ and
$\vdash w : E$, we have that $||v|| = ||w||$, we say that $E$ is ambiguous. In a
plain manner we say that $E$ is ambiguous if there exists a string that can be
parsed under $E$ in more than one way.

A regular expression consisting of nested alternations is referred to as a \emph{sum}.
All subtrees in a sum which is not itself a sum is referred to as a \emph{branch}.

\begin{example}
Let $E = (a + (b + c \times d)) + (a + b)^{*}$; we say that $E$ is a sum with
the branches $a$, $b$, $c \times d$ and $(a + b)^{\ast}$.
\end{example}

\chapter{Converting XML schema to regular expression}
\label{chap:convert}

Regular expressions can be generated from XML schemas. The simples algorithm is
to generate a regular expression for the root element and then inlining the
regular expressions of the child elements.

For example if a schema defines the elements \verb@A@, \verb@B@ and \verb@C@
where \verb@A@ is the root element having \verb@B@ and \verb@C@ as children in
order and \verb@C@ can appear zero or more times, then a regular expression for
the root element would be \verb@<A></A>@ and when inlining the regular
expression for the children we get
\verb@<A><B></B>(<C></C>)*</A>@\footnote{where characters are literals and two
  characters next to each other are the concatenation of the two.}.

  However there are some minor problems with this simple algorithm:

  \begin{itemize}
  \item A schema can define multiple top level elements (also called global
    elements) it is impossible to know which one is intended to be the root
    element. So the actual XML file must be analysed to get the name of the root
    tag in the particular instance.

  \item Schema definitions does not specify the actual layout of the XML file so
    the regular expression needs to handle any number of whitespace where
    allowed in the XML file\footnote{For example are the following valid XML
      \texttt{<Root{\textvisiblespace}att="Foo"\textvisiblespace%
        \textvisiblespace\textbackslash{n}\textvisiblespace>\textvisiblespace%
        </Root\textvisiblespace\textvisiblespace>}}.

    This could however be solved if the original structure of the XML file was
    not guaranteed to be preserved. This way it could be stripped of any
    excessive whitespace, only leaving one space between element names and
    attributes and any whitespace in the element content.
  \end{itemize}

\subsection{Local and global schema definitions}
\label{sec:local-global-schema-definitions}
Different schema types have different ways of defining elements, child elements
and attribute lists of elements. In general the element and child element
definitions can be divided into two principles\footnote{At least this is true
  for XSD, Relax NG schemas and probably others, but DTD only have global
  definitions.}.

\begin{description}
\item[Local definitions] is when the definition of an element's child elements is
  inlined as a child of the element definition.

  \begin{example}[XSD schema using local definitions] \ % end the line
\begin{verbatim}
    <xsd:element name="Book">
      <xsd:complexType>
        <xsd:sequence>
          <xsd:element name="Title" type="xsd:string"/>
           <xsd:element name="Author" type="xsd:string"/>
        </xsd:sequence>
      </xsd:complexType>
    </xsd:element>
\end{verbatim}
  \end{example}

\item[Global definitions] is when the definition of an element's child elements
  is defined at toplevel, and then referenced by the element. This way multiple
  elements can share the same child element definition, but the child element is
  only defined once.

  \begin{example}[XSD schema using global definitions] \ % end the line
\begin{verbatim}
    <xsd:element name="Title" type="xsd:string"/>

    <xsd:element name="Author" type="xsd:string"/>

    <xsd:element name="Book">
      <xsd:complexType>
        <xsd:sequence>
          <xsd:element ref="Title"/>
          <xsd:element ref="Author"/>
        </xsd:sequence>
      </xsd:complexType>
    </xsd:element>
\end{verbatim}
  \end{example}

\end{description}

And of cause any mixture of the two ways of defining elements.

\subsection{Optimising representation by global definitions}

The size of the resulting regular expression can potentially be a problem, for
large schema definitions. The problem arises especially when the data is small
as the total size is the size of the regular expression and the encoded data
combined.

Using global definitions we can just generate a regular expression once for a
child element that potentially gets references multiple times and then inline
them when referenced if the actual regular expression is needed, thus saving
space. The same applies to attributes.
\label{sec:global-definitions-save-space}

Schemas define different types of data that elements and attributes may
contain. An example is the \texttt{\#PCDATA} defined in DTD schemas that
represents parsed character data which basically is all ASCII
characters\footnote{except \&, < and > which should be encoded as entities:
  \&amp;, \&lt; and \&gt; respectively. If not \& is considered an entity
  reference and should be expanded and < and > is considered a tag and should be
  parsed.}\footnote{obviously depending on the file encoding. For simplicity we
  assume ASCII.}. Thus it is important not to expand the \texttt{\#PCDATA}
``type'' until the resulting regular expression is needed as it would expand to
a alternation of all 255 ASCII characters inside each element or attribute that
is defined as being \texttt{\#PCDATA}. Depending on what schema used simple
types should be represented by some distinct symbol that can be expanded to its
actual meaning when the resulting regular expression is needed, to save space.

\subsection{Non-regular schemas}

Most XML schema definitions allow element references (possibly to itself) which
makes it possible to describe a non regular XML file. Most common example is
when a tag may contain itself as a child element.

Obviously it is not possible to create regular expressions from such schema
instances and thus the generation will go into infinite loop unless this is
checked.

\chapter{$\mu$-Recursion}
\label{chap:mu-recursion}

We introduce tail-recursive $\mu$-types as described in \cite{heni2010}.

Regular expressions offer a very restricted form of recursion namely the Kleene
star. This has an impact when we consider the bit code produced from a
particular parse tree.

To see the problem with the Kleene star from a bit coding perspective consider
the unfolding of $E^{\ast}$. It is $1 + E \times E^{\ast}$. It only takes one
bit to code the $1$. But for each Kleene star the $1$-branch is only taken
once. On the other hand each bit coding of $E$ is prefixed with one extra
bit.

If we could trade a longer code for $1$ for shorter codes for $E$ we expect to
gain higher compression rates.

Therefore we generalise recursion to tail-recursive $\mu$-types.

\section{Definition}
Let the language of expressions $UnReg_{\Sigma}^{\mu}$ over a finite alphabet
$\Sigma = \{a_1, \ldots, a_n\}$ be
\[
\alpha ::= 0 \; | \; 1 \; | \; a \; | \; \alpha_1 + \alpha_2 \; | \; \alpha_1
\times \alpha_2 \; | \; \mu X. \alpha \; | \; X
\]

The free variables of $\alpha$ is the set of variables $X$ in $\alpha$ that is
not guarded by $\mu X$. If $\alpha$ has no free variables it is closed. The
expression $\alpha$ is tail-recursive if $\alpha_1$ is closed in all
subexpressions of the form $\alpha_1 \times \alpha_2$.

The language $Reg_{\Sigma}^{\mu}$ is the closed tail-recursive expressions of
$UnReg_{\Sigma}^{\mu}$.

Of course we need inhabitation rules for this new kind of expressions. But we
can reuse the rules for regular regular expressions except for the fold-rule
which now need to handle $\mu$-expressions. It becomes
\[
\inference{v : \alpha \lbrack \mu X. \alpha / X \rbrack}{\mathtt{fold}\ v : \mu X. \alpha}
\]
where the substitution is capture avoiding.

Theorem 9 in \cite{heni2010} states that $Reg_{\Sigma}^{\mu}$ describes exactly
the same languages as $Reg_{\Sigma}$:\fixme{Fixed the cite, check it is the right one}

\begin{enumerate}
\item $\forall E \in Reg_{\Sigma} : \exists \alpha \in Reg_{\Sigma}^{\mu} :
  \{||v|| | |- v : E\} = \{||v|| | |- v : \alpha \}$
\item $\forall \alpha \in Reg_{\Sigma}^{\mu} : \exists E \in Reg_{\Sigma} :
  \{||v|| | |- v : \alpha\} = \{||v|| | |- v : E \}$
\end{enumerate}

And the proof of $1$ tells us how to convert a regular expression into an
equivalent one using $\mu$-recursion: Replace every $E^{\ast}$ with $\mu X.1 +
\alpha \times X$ where $\alpha$ is the conversion of $E$.

\section{Bit coding}

We extend the rules for producing bit codes to closed tail-recursive
$\mu$-expressions. They are given in \fref[plain]{fig:code} and
\fref{fig:decode}. Notice that there a no rules for $X$ as $Reg_{\Sigma}^{\mu}$
only contains closed expressions.

\begin{figure}
\[
\begin{array}{lcl}
\mathtt{code}(\mathrm{()} : 1) & = & \epsilon \\
\mathtt{code}(a : a) & = & \epsilon \\
\mathtt{code}(\mathrm{inl} \, v : \alpha_1 + \alpha_2) & = & 0 \; \mathtt{code}(v : \alpha_1) \\
\mathtt{code}(\mathrm{inr} \, v : \alpha_1 + \alpha_2) & = & 1 \; \mathtt{code}(v : \alpha_2) \\
\mathtt{code}((v,w) : \alpha_1 \times \alpha_2) & = & \mathtt{code}(v : \alpha_1) \; \mathtt{code}(v : \alpha_2) \\
%\mathtt{code}(\mathrm{fold} \, v : E^{*}) & = & \mathtt{code}(v : 1 + E \times E^{*}) \\
%\mathtt{code}(v : X) & = & \mathtt{code}(v : )
\mathtt{code}(\mathrm{fold} \, v : \mu X . \alpha) & = & \mathtt{code}(v : \alpha[\mu X . \alpha / X])
\end{array}
\]
\caption{Type directed encoding from proof values to bit sequences.}
\label{fig:code}
\end{figure}

\begin{figure}
\[
\begin{array}{lcl}
\mathtt{decode'}(d, 1) & = & ((), d) \\
\mathtt{decode'}(d, a) & = & (a, d) \\
\mathtt{decode'}(0d, E + F) & = & \mathbf{let} \; (v,d') = \mathtt{decode'}(d,E) \; \mathbf{in} \; (\mathrm{inl} \, v, d') \\
\mathtt{decode'}(1d, E + F) & = & \mathbf{let} \; (w,d') = \mathtt{decode'}(d,F) \; \mathbf{in} \; (\mathrm{inl} \, w, d') \\
\mathtt{decode'}(d, E \times F) & = &
    \mathbf{let} \left\{ \begin{array}{rcl} (v,d') &=& \mathtt{decode}'(d, E) \\ (w, d'') &=& \mathtt{decode}'(d', F) \end{array} \right\}
    \mathbf{in} \; ((v,w), d'') \\
%\mathtt{decode'}(d, E^{*}) & = & \mathbf{let} \; (v,d') = \mathtt{decode'}(d : 1 + E \times E^{*}) \; \mathbf{in} \; (\mathrm{fold} \, v, d')\\
\mathtt{decode'}(d, \mu X . \alpha) = \mathbf{let} \; (v,d') & = & \mathtt{decode'}(d, \alpha[\mu X . \alpha / X]) \; \mathbf{in} \; (\mathrm{fold} \, v, d') \\
\\
\mathtt{decode}(d, E) & = & \mathbf{let} \; (v, d') = \mathtt{decode'}(d, E) \; \mathbf{in} \; v
\end{array}
\]
\caption{Type directed decoding from bit sequences to proof values.}
\label{fig:decode}
\end{figure}

\section{Normalisation}
One gains nothing just by converting a regular expression to an equivalent
$\mu$-recursive one by the rule given above. Consider an example: bit code the
string $aab$ under the regular expression $(a + (b + c))^{\ast}$. The bit codes
for $a$ and $b$ under $(a + (b + c))$ being $0$ and $10$ respectively we get
$10101100$.

Now bit code the same string under the equivalent $\mu X. 1 + (a + (b + c))
\times X$. Again the result is $10101100$.

But observe that $\mu X. 1 + (a + (b + c)) \times X = \mu X. a \times X + (b
\times X + (c \times X + 1))$. Bit coding $aab$ with regard to the latter yields
the code $0010$. That is a reduction of 50\%.

The reason why the bit code under the latter regular expression is shorter is
that the $1$ is buried inside the sum. That way we don't use a full bit per
character to code that the string is not finished yet.

It is easy to automatically balance a sum (see \fref{chap:huffman}) but that
doesn't immediately help as the ``$\times X$'' prevents us from including the
$1$ in the balancing.

Luckily we can exploit the distributivity of $\times$ with respect to $+$. We
rewrite $\mu$-recursive expressions to their normal form using the system of
inference rules given in \fref{fig:normal-form}. We write $|- \alpha ->n
\alpha'$ if $\alpha ->n \alpha'$ is derivable in the system.

\begin{figure}
\[
dist_1 : \inference{\gamma \times \alpha + \gamma \times \beta ->n \delta}{\gamma \times
  (\alpha + \beta) ->n \delta} \qquad
dist_2 : \inference{\alpha \times \gamma + \beta \times \gamma ->n \delta}{(\alpha +
  \beta) \times \gamma ->n \delta}(\gamma \neq \gamma_1 + \gamma_2)
\]

\[
sum : \inference{\alpha ->n \alpha' & \beta ->n \beta'}{\alpha + \beta ->n \alpha' +
  \beta'} \qquad
rec : \inference{\alpha ->n \alpha'}{\mu X. \alpha ->n \mu X. \alpha'}
\]

\[
prod : \inference{\alpha ->n \alpha' & \beta ->n \beta'}{\alpha \times \beta ->n \alpha'
  \times \beta'}(\alpha \neq \alpha_1 + \alpha_2 \land \beta \neq \beta_1 + \beta_2)
\]

\[
sym : \inference{}{a ->n a} \qquad
one : \inference{}{1 ->n 1} \qquad
zero : \inference{}{0 ->n 0} \qquad
var : \inference{}{X ->n X}
\]
\caption{Conversion to normal form.}
\label{fig:normal-form}
\end{figure}

Informally normal forms have the following properties which we show below.
\begin{enumerate}
\item The language $Reg_{\Sigma}^{\mu}$ is closed under normal form conversion.
\item The normal form conversion gives way to a total function $norm :
  Reg_{\Sigma}^{\mu} -> Reg_{\Sigma}^{\mu}$.
\item An expression is equivalent to it's normal form. They describe the same
  language.
\item The bit code size does not blow up under normal forms.
\end{enumerate}

\begin{theorem}
  For all $\alpha \in Reg_{\Sigma}^{\mu}$ and $\alpha'$ such that $|- \alpha ->n
  \alpha'$ we have that $\alpha' \in Reg_{\Sigma}^{\mu}$.

  \begin{proof}
    It is immediately clear that $\alpha' \in UnReg_{\Sigma}^{\mu}$. So we need
    to show that $\alpha'$ is closed and tail-recursive.

    To see that it is closed notice that the rules never move anything past a
    $\mu X$ and that variables are not added or renamed. This property does not
    rely on tail-recursiveness so we can use it as a lemma in itself.

    To show that $\alpha'$ is tail-recursive we need to show that $\alpha'_1$ is
    closed in all subexpressions of the form $\alpha'_1 \times \alpha'_2$.
    For each such subexpressions there must be a subderivation in the derivation(s)
    of $\alpha ->n \alpha'$ of the form
    \[
    \inference{\mathcal{E} & \mathcal{F} \\ \beta ->n \beta' & \gamma ->n
      \gamma'}{\beta \times \gamma ->n \beta' \times \gamma'}
    \]
    From the derivations $\mathcal{E}$ and $\mathcal{F}$ we have $|- \beta ->n
    \beta'$ and $|- \gamma ->n \gamma'$ respectively. We also know from the
    tail-recursiveness of $\alpha$ that $\beta$ is closed. So by the lemma
    stated above we must have that $\beta'$ is also closed which in turn means
    that $\alpha'$ is tail-recursive.
  \end{proof}
\end{theorem}

\begin{theorem}
  Define $norm(\alpha) = \alpha'$ where $|- \alpha ->n \alpha'$. Then $norm :
  Reg_{\Sigma}^{\mu} -> Reg_{\Sigma}^{\mu}$ is indeed well-defined and total.

  \begin{proof}
    We need to show three things about the rules for $->n$:
    \begin{enumerate}
    \item Termination. There are no infinite derivations.
    \item Completeness. For every $\alpha$ there exists a derivation $\alpha ->n
      \alpha'$.
    \item Determinism. For every $\alpha$ there exists at most one derivation
      $\alpha ->n \alpha'$.
    \end{enumerate}
    \subsubsection{Termination}
    Proof by induction on the structure of $\alpha$.

    The only rules whose premises left hand sides are not strictly smaller than
    the conclusions left hand side are $dist_1$ and $dist_2$. So we need to show
    that these rules cannot result in an infinite derivation. They are similar
    so we only show it for $dist_1$. A subderivation $\mathcal{E}$ ending in
    $dist_1$ must have the form
    \[
    \inference{\inference{\mathcal{F} & \mathcal{G} \\ \gamma \times \alpha ->n
        \delta_1 & \gamma \times \beta ->n \delta_2 }{\gamma \times \alpha +
        \gamma \times \beta ->n \delta_1 + \delta_2}}{\gamma \times (\alpha +
      \beta) ->n \delta}
    \]
    The left hand side of the conclusions of $\mathcal{F}$ and $\mathcal{G}$ are
    strictly smaller than the left hand side of the conclusion of $\mathcal{E}$.

    \subsubsection{Completeness}
    Comparing the language of $UnReg_{\Sigma}^{\mu}$ to the rules and noting
    that $Reg_{\Sigma}^{\mu} \subseteq UnReg_{\Sigma}^{\mu}$ we see that the rules
    are complete.

    \subsubsection{Determinism}
    The rules $dist_1$, $dist_2$ and $prod$ all end in $\times$ but the
    side-conditions ensure that only one of them is applicable at any time. So
    the rules are deterministic.

    This concludes our proof.
  \end{proof}
\end{theorem}

\begin{theorem}
  If $norm(\alpha) = \alpha'$ then $\alpha = \alpha'$.

  \begin{proof} (Sketch).
    Since the inference rules for normal form conversion are defined for
    possibly non-closed expressions we need to prove a stronger property. Namely
    for all $\alpha_i \in Reg_{\Sigma}^{\mu}$ and derivations $\alpha ->n \alpha'$
    then $\alpha \lbrack \alpha_i / X_i \rbrack = \alpha' \lbrack \alpha'_i /
    X_i \rbrack$ where the $X_i$'s are the free variables of $\alpha$ and
    $\alpha_i ->n \alpha'_i$ is derivable. It is easy to see that the free
    variables of $\alpha$ and $\alpha'$ are the same.

    To prove the theorem we would need a system for proving equivalence among
    tail-recursive closed $\mu$-expressions. Alas we do not have such a system
    but we speculate that the system of \cite{heni2010} is still sound and
    complete with the additional rule\fixme{fixed the cite, is it correct?}
    \[
    rec : \mu X. \alpha = \alpha \lbrack \mu X. \alpha / X \rbrack
    \]
    Noticing that the free variables of $\mu X. \beta$ is a subset of the free
    variables of $\beta$ the result follows by structural induction on $\alpha$.
  \end{proof}
\end{theorem}

\begin{theorem}
  Conversion to normal form preserves bit coding size.

  Since regular expressions are, in general, ambiguous the property we are after
  here is not so obvious; Again we need to handle possible non-closed
  expressions. Given an expression $\alpha$ let $\hat{\alpha} = \alpha \lbrack
  \alpha_i / X_i \rbrack$, $\hat{\alpha}' = \alpha' \lbrack \alpha'_i / X_i
  \rbrack$ where the $X_i$'s are the free variables of $\alpha$, the
  $\alpha_i$'s are arbitrary expressions in $Reg_{\Sigma}^{\mu}$, and $\alpha
  ->n \alpha'$ and $\alpha_i ->n \alpha'_i$ are derivable.

  Now if $v$ is a proof of membership in $\hat{\alpha}$ with the bit coding
  $\mathfrak{b}$ then there exists a membership proof $v'$ in $\hat{\alpha}'$
  with the bit coding $\mathfrak{b'}$ such that $| \mathfrak{b'} | \leq |
  \mathfrak{b} |$.

  \begin{proof}
    With the problem properly formulated the proof itself is almost trivial as
    is often the case.

    Again we use structural induction on $\alpha$. The interesting cases are
    $dist_1$, $dist_2$ and $rec$. The cases for $dist_1$ and $dist_2$ are
    analogous so we give here only the first one.\\[1em]


    Case $dist_1$: By the induction hypothesis we have that if the bit coding of
    $w : (\gamma \times \alpha + \gamma \times \beta)\lbrack \alpha_i / X_i
    \rbrack$ is $\mathfrak{c}$ then $| \mathfrak{b'} | \leq | \mathfrak{c}
    |$. Now we make a case distinction on the first bit $\mathfrak{c_0}$ of
    $\mathfrak{c}$. In either case the first part of the remaining bits
    $\mathfrak{c_1}$ codes a membership proof of $\gamma\lbrack \alpha_i / X_i
    \rbrack$. If $\mathfrak{c_0}$ is $0$ then the rest of the bits
    $\mathfrak{c_2}$ code at membership proof of $\alpha\lbrack \alpha_i / X_i
    \rbrack$, and if it is $1$ then $\mathfrak{c_2}$ codes a membership proof of
    $\beta\lbrack \alpha_i / X_i \rbrack$. In either case we can reorder the bit
    code to construct one that codes a membership proof of $(\gamma \times
    (\alpha + \beta))\lbrack \alpha_i / X_i \rbrack = \hat{\alpha}$, namely
    $\mathfrak{b} = \mathfrak{c_1}\mathfrak{c_0}\mathfrak{c_2}$.\\[1em]

    Case $rec$: The
    proof follows by noticing that the free variables of $\mu X. \alpha$ is a
    subset of the free variables of $\alpha$.
  \end{proof}

  Alternation is the sole reason for ambiguity, so a parser can be made
  unambiguous by always choosing left over right or vice versa.

  It is not too hard to see that the bit coding size is preserved exactly
  (eg. $| \mathfrak{b'} | = | \mathfrak{b} |$) when using such a parser.
\end{theorem}

\chapter{Specialising regular expressions using Huffman trees}
\label{chap:huffman}

The fact that the alternation operator in $Reg_{\Sigma}$ is associative and
commutative with respect to language equivalence can be exploited for encoding
strings more efficiently. Consider the string \texttt{"aaa"}; compressing it
using the expression $E_1 = ((a + b) + c)^{*}$ yields the bit sequence
$111111110$. On the other hand, compressing it using the expression $E_2 = (a +
(b + c))^{*}$ yields the bit sequence $111110$. As $\mathcal{L}|[E_1|] =
\mathcal{L}|[E_2|]$ both expressions can be used for compressing the exact same
set of strings; for the specific string \texttt{"aaa"} we prefer $E_2$. For an
arbitrary string, $s$, and an arbitrary regular expression, $E$, where $s \in
\mathcal{L}|[E|]$, the optimal method for reordering the sums in $E$, with
respect to the compression ratio, is to reorder them as Huffman trees based on
$s$.

To do that, we introduce the syntactic form $Reg'_\Sigma$ over the finite
alphabet $\Sigma = {a_1, \dots, a_n}$:

\[
    E ::= 0 \, | \, 1 \, | \, a \, | \, \Sigma(E_1, E_2, E_3, \dots, E_n) \, | \, E_1 \times E_2 \, | \, E^{*}
\]

The operator $\Sigma(E_1, E_2, E_3, \dots, E_n)$ denotes a sum of alternations,
and the rest of the symbols denotes the same as in $Reg$. As $Reg'$ will only be
used as a temporary syntactic form for reordering the sums, the order of the
branches inside a $\Sigma$ is insignificant. Next we introduce what we choose to
call the \emph{flattened form} of an expression $E$; to bring $E$ into the
\emph{flattened form}, $E'$, we flatten out all nested sums in $E$ and rewrite
them in $Reg'$ using the sum operator $\Sigma$; i.e. $(a + (b \times c + d) + e
\times f)^{*} \times (c + d)$ will be rewritten into $\Sigma(a, b \times c, d, e
\times f)^{*} \times \Sigma{(c, d)}$.

After having flattened out an expression $E$ into $E'$, we pick a parse tree
$v$ given the input string $s$, such that $|- v : E$ and $||v|| = s$; by
running trough the parse tree, we can for each sum $\Sigma(E_1, \dots, E_n)$ in
$E'$ count how many times $E_k, k \in 1 \dots n$ is selected in the parse tree.

Finally, we can for each sum $\Sigma$ in $E'$ use these countings to reorder the
$\Sigma$ back into the syntactic form of $Reg$, by ordering the alternations as
Huffman trees.

\begin{example}
  Consider the string $s = \mathtt{"abbc"}$ and the regular expression $E = (a +
  ((b + c) + d))^{*}$. Parsing $s$ using $E$ yields the following parse tree:
\[
\begin{array}{rcl}
  v & = & \mathtt{fold}(\mathtt{inr}(\mathtt{inl} \; a, \\
    &   & \mathtt{fold}(\mathtt{inr}(\mathtt{inr}(\mathtt{inl}(\mathtt{inl} \; b), \\
    &   & \mathtt{fold}(\mathtt{inr}(\mathtt{inr}(\mathtt{inl}(\mathtt{inl} \; b), \\
    &   & \mathtt{fold}(\mathtt{inr}(\mathtt{inr}(\mathtt{inl}(\mathtt{inr} \; c), \\
    &   & \mathtt{fold}(\mathtt{inl}(()))))))))))))))
\end{array}
\]
\noindent Encoding $s$ using $E$ gives the bit sequence $101100110011010$ (15
bits). Now when translating $E$ into $Reg'$ we get the expression $E' =
\Sigma{(a, b, c, d)}^{*}$. Counting the number of times each path in the sum
$\Sigma{(a, b, c, d)}$ is taken in $v$ we get:

\begin{center}
\begin{tabular}{c|c|c|c}
$a$ & $b$ & $c$ & $d$ \\
\hline
1   & 2   & 1   & 0
\end{tabular}
\end{center}

\noindent Finally by reordering the sum into an Huffman tree using the above
frequencies we get the $Reg$-expression $F = (b + (a + (c + d)))^{*}$. Encoding
$s$ using $F$ gives the bit sequence $110101011100$ (12 bits); thus we have
gained 3 bits.

\end{example}

\chapter{Implementation}


\section{DTD schema parser}

We have implemented a DTD schema parser, even though it limits the possibilities
of optimisation by restricting specific types on elements contained data (e.g.,
only integers or only dates).

Our decision to do this anyway was that the DBLP\footnote{The DBLP Computer
  Science Bibliography \url{http://www.informatik.uni-trier.de/~ley/db/}}
database comes with a DTD
schema\footnote{\url{http://www.informatik.uni-trier.de/~ley/db/about/dblp.dtd}}
ready to use. This is handy as the DBLP database can be fetched in at least four
versions: an up to date, 2009, 2006 and 2004 version, each getting bigger
up until the current up to date version.

We utilise the HaXML Haskell package to parse the DTD file and then we transform
this into an extended regular expression data type. Currently we only support as
much as the DTD syntax that was needed to parse the DBLP's DTD schema which at
least includes most of elements (element content and mixed content) and
attribute definitions.

The extended regular expression adds query (\texttt{?}: zero or more), plus
(\texttt{+}: one or more) and some character classes, especially one for
representing the \texttt{\#PCDATA} type. This extended regular expression data
type is intended as being the high level representation used when saving the
regular expression with the encoded data so it is as small as possible.

\section{Splitting of XML files}
\label{sec:splitting-xml-files}

After seeing how good GZIP was at compressing the original XML file, we came up
with the idea of splitting the file by pulling out all the data and leaving only
the meta ( markup).

The initial idea is that this is saved into one file as the XML file is parsed,
putting the data in sequence of how they appear in the XML file, and then
leaving an index in the XML file of the position where the data was put in the
data file.

However as statistical compression algorithms ideally are better at compressing
when it encounters large chunks of similar data, we group the data from the XML
file such that all attributes with the same name and all data from elements with
the same name gets its data grouped together. This is especially useful for
attributes or elements that contains dates or years.

However in our implementation of this we do not save the indexes where the data
gets placed in the data file so we have some loss of information that is needed
if the compressed file should be uncompressed later.

We have used HXT for the XML parsing.

\chapter{Experimental results}
\label{chap:experimental_results}


We have used a small sample version of the DBLP XML file which we have created
for the purpose. We have compared our result to the standard Unix tool GZIP
version 1.3.12 and our bit coding.

As seen in \fref{tab:experimental-results} we do not perform as well as GZIP in
either of the two ways we have tested our bit coding. Actually we end up with a
compression that is roughly 33\% worse when comparing the split method and GZIP.
See \ref{tab:experimental-results-comparison} for the complete list. \fixme{Get
  the percentage correct}


\fixme{Tell about the gzip of the only the markup}

\paragraph{Note.} The sum of the size of the two splits are not exactly the same
as the size of the original file. This is described in the caption of
\fref{tab:experimental-results}

\begin{nonfloatingtable}
  \centering
  
  \begin{tabular}{|l|l|}\hline
    \textbf{Test \#} & \textbf{File} \\ \hline
    1 & DBLP sample file \#1 \\
    & \begin{tabular}{|l|c|c|} \hline 

      \textbf{Description} & \textbf{Original size} & \textbf{Compressed size } \\ \hline

      Original file (GZIP)  & 1069858 & 177541 \\ 
      Original file (bit code)  & 1069858 & 373568 \\ \hline
      Orig split into data (GZIP) & 606046 & 144779  \\ \hline
      Orig split into meta (GZIP) & 457985 & 5622 \\
      Orig split into meta (bit code) & 457985 & 94745 \\ \hline
    \end{tabular} \\
    & \\ \hline
  \end{tabular}

  \caption{Results from compressing using GZIP and our bit coding. All sizes are
    in bytes. \textbf{Note} that the sum of the size of the two splits are
    not exactly the same as the size of the original file as the XML parser
    strips any unnecessary extra whitespace (i.e., multiple spaces between
    element attributes) and the \texttt{<!DOCTYPE..>} element was also removed
    when the meta data file was generated. Also indexes should have been placed
    into the meta file, making it bigger (see \fref{sec:splitting-xml-files})}
  \label{tab:experimental-results}
\end{nonfloatingtable}

\begin{nonfloatingtable}
  \centering
  \begin{tabular}{|l|l|} \hline
    \textbf{Test \#} & \textbf{File} \\ \hline
    1 & DBLP sample file \#1 \\
    & \begin{tabular}{|l|c|c|c|} \hline
    \textbf{Description} & \textbf{Orig. size} & \textbf{Comp. size} &
    \textbf{Comp. factor} \\ \hline

    Orig file GZIP & 1069858 & 177541 & 6.03 \\
    Orig file bit code & 1069858 & 373568 & 2.86 \\ \hline
    Splittet file (GZIP data, bit code meta) & 1064031 & 239524 & 4.44 \\
    Splittet file (GZIP data, GZIP meta) & 1064031 & 150401 & 7.07 \\ \hline
  \end{tabular} \\ 
   & \\ \hline
  \end{tabular}

  \caption{Comparison of the results from \fref{tab:experimental-results}. All
    sizes are in bytes }
\label{tab:experimental-results-comparison}
\end{nonfloatingtable}


% XML: 1069858 -> GZIP -> 177541
% 
% data: 606046 -> GZIP -> 144779 }
%                                } -> 239524
% meta: 457985 -> regp -> 94745  }

\chapter{Conclusion}
\label{chap:conclusion}

We have recounted and extended the technique described in \cite{heni2010} for
bit coding parse trees under regular expressions. Further we have shown how to
use this technique for compressing XML files by automatically deriving regular
expressions from (non-recursive) XML schemes. Finally we have implemented a
prototype of an XML compressor based on our approach and conducted a test using
the prototype on data taken from the Digital Bibliography \& Library Project.

Our approach suffers from the following drawbacks:

\begin{itemize}
\item Existing parsing techniques for regular expressions are less efficient in
  time or space (or both) than the statistical methods used in for example GZIP
  which runs in linear time and constant space \cite{de1996}.
\item Our method is dependent on the presence of a non-recursive XML schema
  whereas GZIP is able to compress arbitrary XML files.
\item In our experiment GZIP has a better compression ratio than our prototype.
\end{itemize}

We believe that domains exist which are suitable candidates for the techniques
that we describe in this paper; however, based on the above mentioned facts we
conclude that XML file compression isn't such a domain.

One of the advantages of our method is that we bit code not strings but parse
trees. This is not particularly useful in the case of XML but other areas might
benefit from it. Since parse trees contain more information than their
flattenings it is not surprising that our method performs worse than say GZIP.

\fixme{Experimental results not accurate because of low number of tests}
\fixme{it sux compared to GZIP}

\section{Further work}

Below is a list of possible further work.

\begin{description}
\item[Refactoring.] As it is not all schema instances that utilises global
  definitions (see \label{sec:local-global-schema-definitions}), it would make
  sense to extract such child elements and attributes into being global
  declarations instead as long as the result is not bigger than the
  original\footnote{We need to remember that some sort of reference needs to be inserted
    which also takes some space.}.


\item[Proper parser.] The experiments we have made should be rerun using a
  parser that can handle substantial larger files. Our parser began to struggle
  with file sizes about 1MB (took >200minutes).

\item[Comparing results.] It would be worth comparing results not just with GZIP
  but at least also with XMill\cite{lisu2000}.

\item [Splitting meta and data.] It seems that statistical compression
  algorithms like the one used by GZIP performs quite better when compressing
  the meta and the data separated. It certanly does it better than our bit
  coding so it would be worth wile looking more into this. Maybe compare this to
  the way XMill works.

\end{description}

\clearpage

\bibliographystyle{../bibliography/theseurl}
% \bibliographystyle{plain}
\bibliography{../bibliography/bibliography}

\end{document}

%%% Local Variables: 
%%% mode: tex
%%% Tex-master: t
%%% End:
